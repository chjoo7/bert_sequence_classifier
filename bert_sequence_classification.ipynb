{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_YTfDPSga_F"
   },
   "source": [
    "# Document Classification Using KR-SBERT via Transformers\n",
    "\n",
    "- Check the accuracy of model to apply our pre-trained KoRean S-BERT model to a document classification task, using HuggingFace's `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 18 01:59:53 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA A100-PCIE-40GB           Off| 00000000:41:00.0 Off |                    0 |\r\n",
      "| N/A   35C    P0               38W / 250W|      0MiB / 40960MiB |      0%      Default |\r\n",
      "|                                         |                      |             Disabled |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv5o7uZmcQAB"
   },
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QZFeNxok95F8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting kss\n",
      "  Downloading kss-4.5.4.tar.gz (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: filelock in /home/jovyan_venv/.venv/torch2.0.0-py3.10-cuda11.8/lib/python3.10/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/jovyan_venv/.venv/torch2.0.0-py3.10-cuda11.8/lib/python3.10/site-packages (from sentence-transformers) (2.0.0+cu118)\n",
      "Requirement already satisfied: torchvision in /home/jovyan_venv/.venv/torch2.0.0-py3.10-cuda11.8/lib/python3.10/site-packages (from sentence-transformers) (0.15.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.1)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting emoji==1.2.0\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pecab\n",
      "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan_venv/.venv/torch2.0.0-py3.10-cuda11.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/jovyan_venv/.venv/torch2.0.0-py3.10-cuda11.8/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: lit in /home/jovyan_venv/.venv/torch2.0.0-py3.10-cuda11.8/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
      "Requirement already satisfied: cmake in /home/jovyan_venv/.venv/torch2.0.0-py3.10-cuda11.8/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.26.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: pyarrow in /home/jovyan_venv/.venv/torch2.0.0-py3.10-cuda11.8/lib/python3.10/site-packages (from pecab->kss) (12.0.1)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan_venv/.venv/torch2.0.0-py3.10-cuda11.8/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Building wheels for collected packages: sentence-transformers, kss, pecab\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=8a8e1e76b1cf3e8d0122acc9de41ef23ee9a52e7ecde666b5d1970615bdaf879\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "  Building wheel for kss (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kss: filename=kss-4.5.4-py3-none-any.whl size=54467 sha256=f97a8ce620b333055c0358f31383a3d5c33ce091251059c7a6f19ad6d39e77c1\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/61/7b/ba/e620ef5d96a61cdd83bdee4c2bb4aec8a74de5d72fcbb00e80\n",
      "  Building wheel for pecab (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646666 sha256=e1b93f4cd3ea5303b77b66a0cc7dd603feb5c5cef70b8dabaf6414516e2aab31\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n",
      "Successfully built sentence-transformers kss pecab\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, emoji, regex, fsspec, pecab, nltk, huggingface-hub, transformers, kss, sentence-transformers, accelerate\n",
      "Successfully installed accelerate-0.21.0 emoji-1.2.0 fsspec-2023.6.0 huggingface-hub-0.16.4 kss-4.5.4 nltk-3.8.1 pecab-1.0.8 regex-2023.6.3 safetensors-0.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.30.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers sentence-transformers kss accelerate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udlz6OUqcYYy"
   },
   "source": [
    "### BNC dataset\n",
    "\n",
    "Download the Balanced News Corpus for a sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RAhBKWOlpBYJ"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (3846666902.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    !unzip BalancedNewsCorpusShuffled.zip\u001b[0m\n\u001b[0m                                         \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Lg2jL89n3lqkKCulAnk4WwmI8G1hNfIA' -O BalancedNewsCorpusShuffled.zip\n",
    "!unzip BalancedNewsCorpusShuffled.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxOq4BMKeetn"
   },
   "source": [
    "## 1. Setting on Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vNpNL8SQ836P"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For Transformer models\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For train/dev/test datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "# For evaluation\n",
    "from torch import manual_seed\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntyRXOlFvz6Y"
   },
   "source": [
    "Let us load a `SentenceTransformer` model for sentence embddings and a `BertForSequenceClassification` for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "eMCnjMGJ--in"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6297cef870>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "sbert_model = SentenceTransformer(sbert_model_name)\n",
    "config = sbert_model._first_module().auto_model.config # for bert token embeddings\n",
    "#from transformers import BertConfig\n",
    "#config = BertConfig()\n",
    "config.num_labels=9\n",
    "config.max_position_embeddings = sbert_model.max_seq_length    #128\n",
    "model = BertForSequenceClassification(config)\n",
    "model.main_input_name = 'inputs_embeds'\n",
    "max_seq_length = sbert_model.max_seq_length\n",
    "manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(model.main_input_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "IVuNnOulqJ_3"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p7q5_l5g9Yl"
   },
   "source": [
    "## 2. Building the BNC datasets\n",
    "\n",
    "We define a new `Dataset` class loading the Balanced News Corpus dataset for the `BertForSequenceClassification`model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "DGLhUBxER1Yr"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(text:str):\n",
    "  # https://github.com/YongWookHa/kor-text-preprocess/blob/master/src/clean.py\n",
    "    not_used = re.compile('[^ .?!/@$%~|0-9|ㄱ-ㅣ가-힣]+')\n",
    "    dup_space = re.compile('[ \\t]+')  # white space duplicate\n",
    "    dup_stop = re.compile('[\\.]+')  # full stop duplicate\n",
    "\n",
    "    cleaned = not_used.sub('', text.strip())\n",
    "    cleaned = dup_space.sub(' ', cleaned)\n",
    "    cleaned = dup_stop.sub('.', cleaned)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "DE4QFtPgMft6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from kss import split_sentences # Sentence segmentation for the Korean Language\n",
    "# sent_tokenize = split_sentences\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "NSEFSrxBEf1l"
   },
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(text:str, cls_token='[CLS]', sep_token='[SEP]', padding=True, truncate=True, max_len=128):\n",
    "    sentences = [cls_token] + sent_tokenize(text) + [sep_token]\n",
    "    embeddings = sbert_model.encode(sentences, convert_to_tensor=True)\n",
    "    d = sbert_model.get_sentence_embedding_dimension()\n",
    "    n = len(sentences)\n",
    "\n",
    "    seq_len = n\n",
    "\n",
    "    if padding:\n",
    "        seq_len = max(n, max_len)\n",
    "\n",
    "    if truncate:\n",
    "        seq_len = min(seq_len, max_len)\n",
    "\n",
    "    output = torch.zeros((seq_len, d), dtype=torch.float32).to(device)\n",
    "    for i in range(min(n, seq_len)):\n",
    "        output[i] = embeddings[i]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "aHQHhX-l_e0Q"
   },
   "outputs": [],
   "source": [
    "class BNCDataset(Dataset):\n",
    "\n",
    "    labels = ['IT/과학', '경제', '문화', '미용/건강', '사회', '생활', '스포츠', '연예', '정치']\n",
    "\n",
    "    def __init__(self, data_file='BalancedNewsCorpus_train.csv'):\n",
    "        data = pd.read_csv(data_file)\n",
    "        self.text = data['News'].apply(lambda text: text.replace('<p>', '\\n').replace('</p>', '\\n'))\n",
    "        self.text = self.text.apply(clean).tolist()\n",
    "        self.label = data['Topic'].apply(lambda label: self.labels.index(label)).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        label = torch.tensor(self.label[idx]).to(device)\n",
    "        feature = {'inputs_embeds': get_sentence_embeddings(text), 'labels': label}\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNciffUPwtpL"
   },
   "source": [
    "Load the BNC dataset files we have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./BalancedNewsCorpus_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "      <th>NewsPaper</th>\n",
       "      <th>Topic</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NLRW1900000141</td>\n",
       "      <td>20170324</td>\n",
       "      <td>부산일보</td>\n",
       "      <td>스포츠</td>\n",
       "      <td>&lt;p&gt; 야구 종가, 마침내 정상에 서다 &lt;/p&gt; &lt;p&gt; '야구 종가' 미국이 푸에르...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NPRW1900000003</td>\n",
       "      <td>20110209</td>\n",
       "      <td>한국경제신문사</td>\n",
       "      <td>정치</td>\n",
       "      <td>&lt;p&gt; 외통위 27명중 15명 \"FTA 추가협상안만 처리\" &lt;/p&gt; &lt;p&gt; 국회 외...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NLRW1900000144</td>\n",
       "      <td>20100406</td>\n",
       "      <td>영남일보</td>\n",
       "      <td>사회</td>\n",
       "      <td>&lt;p&gt; 한나라 \"地選후보, 희망연대 당원 구함\" 공천변수 작용 주목 &lt;/p&gt; &lt;p&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLRW1900000064</td>\n",
       "      <td>20100804</td>\n",
       "      <td>광주매일신문</td>\n",
       "      <td>스포츠</td>\n",
       "      <td>&lt;p&gt; 모처럼 살아난 ‘CK포’ 7타점 합작 &lt;/p&gt; &lt;p&gt; KIA 12 3 LG ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NLRW1900000070</td>\n",
       "      <td>20160615</td>\n",
       "      <td>광주매일신문</td>\n",
       "      <td>문화</td>\n",
       "      <td>&lt;p&gt; 亞문화전당서 동방의 등불 만나다 &lt;/p&gt; &lt;p&gt; “일찍이 아시아의 황금 시기...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename      date NewsPaper Topic  \\\n",
       "0  NLRW1900000141  20170324      부산일보   스포츠   \n",
       "1  NPRW1900000003  20110209   한국경제신문사    정치   \n",
       "2  NLRW1900000144  20100406      영남일보    사회   \n",
       "3  NLRW1900000064  20100804    광주매일신문   스포츠   \n",
       "4  NLRW1900000070  20160615    광주매일신문    문화   \n",
       "\n",
       "                                                News  \n",
       "0  <p> 야구 종가, 마침내 정상에 서다 </p> <p> '야구 종가' 미국이 푸에르...  \n",
       "1  <p> 외통위 27명중 15명 \"FTA 추가협상안만 처리\" </p> <p> 국회 외...  \n",
       "2  <p> 한나라 \"地選후보, 희망연대 당원 구함\" 공천변수 작용 주목 </p> <p>...  \n",
       "3  <p> 모처럼 살아난 ‘CK포’ 7타점 합작 </p> <p> KIA 12 3 LG ...  \n",
       "4  <p> 亞문화전당서 동방의 등불 만나다 </p> <p> “일찍이 아시아의 황금 시기...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./BalancedNewsCorpus_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "sAzDMOCHAPJr"
   },
   "outputs": [],
   "source": [
    "train_dataset = BNCDataset('BalancedNewsCorpus_train.csv')\n",
    "test_dataset = BNCDataset('BalancedNewsCorpus_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "u41_9tigLBbs"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(train_dataset, [8100, 900], generator=manual_seed(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_ZPMCuow8O6"
   },
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bYRwah9tAitP"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./bnc-results\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # eval_steps=10,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    dataloader_pin_memory=False, # False for GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzMERJh-xF0Z"
   },
   "source": [
    "We will evaluate our classifier using Accuracy, F1, Precision, and Recall scores. This should be defined as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ns82ofw7_Lp0"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(confusion_matrix(labels, preds))\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6s-xEQJexTjp"
   },
   "source": [
    "Instantiate the `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qTgZbZYMAmCU"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKGL6ICOxBN9"
   },
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wG-6QEQxAo19",
    "outputId": "749746ba-81d0-4b78-b6f0-4921636458ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.venv/torch2.0.0-py3.10-cuda11.8/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [640/640 29:45, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.978445</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.632912</td>\n",
       "      <td>0.675187</td>\n",
       "      <td>0.658610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.744105</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>0.752039</td>\n",
       "      <td>0.756107</td>\n",
       "      <td>0.756831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.680357</td>\n",
       "      <td>0.775556</td>\n",
       "      <td>0.767813</td>\n",
       "      <td>0.774262</td>\n",
       "      <td>0.771219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.626381</td>\n",
       "      <td>0.792222</td>\n",
       "      <td>0.787378</td>\n",
       "      <td>0.792799</td>\n",
       "      <td>0.790302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.651894</td>\n",
       "      <td>0.771111</td>\n",
       "      <td>0.768801</td>\n",
       "      <td>0.772233</td>\n",
       "      <td>0.774228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.584599</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.799083</td>\n",
       "      <td>0.801566</td>\n",
       "      <td>0.801908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586437</td>\n",
       "      <td>0.808889</td>\n",
       "      <td>0.806225</td>\n",
       "      <td>0.807693</td>\n",
       "      <td>0.808273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.573891</td>\n",
       "      <td>0.817778</td>\n",
       "      <td>0.813671</td>\n",
       "      <td>0.813909</td>\n",
       "      <td>0.814402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.576083</td>\n",
       "      <td>0.808889</td>\n",
       "      <td>0.803079</td>\n",
       "      <td>0.804306</td>\n",
       "      <td>0.806536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.806610</td>\n",
       "      <td>0.808552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 42   5  10   4   2   3   2   0   7]\n",
      " [  9  84   0   2   1   1   2   0  11]\n",
      " [  1   1  87   1   1   1   0   4   3]\n",
      " [  4   3   2  70   1   0   1   0   1]\n",
      " [  1  16   4   9  19   4   3   2  47]\n",
      " [  6  18   3  13   5  29   9   1   6]\n",
      " [  0   0   0   1   0   4  92   1   3]\n",
      " [  1   0  45   1   2   0   4  70   3]\n",
      " [  0   8   1   0   2   0   0   0 101]]\n",
      "[[ 60   2   4   1   2   2   0   2   2]\n",
      " [  7  82   0   3   7   3   1   0   7]\n",
      " [  1   0  84   1   2   1   0   8   2]\n",
      " [  5   2   2  68   2   1   1   0   1]\n",
      " [  4   6   4   5  49   7   2   0  28]\n",
      " [  6   9   4   5   8  52   1   1   4]\n",
      " [  0   0   0   1   1   4  92   1   2]\n",
      " [  5   0  16   0   3   0   1 100   1]\n",
      " [  1   6   1   0  10   0   0   0  94]]\n",
      "[[ 60   1   4   0   1   4   1   2   2]\n",
      " [  7  91   0   2   2   2   3   0   3]\n",
      " [  3   1  72   1   3   5   0  12   2]\n",
      " [  6   2   1  69   2   1   1   0   0]\n",
      " [  5  15   3   1  54   5   2   4  16]\n",
      " [  6  14   3   3   4  52   4   2   2]\n",
      " [  0   1   0   1   0   2  95   0   2]\n",
      " [  3   0   1   0   1   0   2 119   0]\n",
      " [  1  10   1   1  11   1   1   0  86]]\n",
      "[[ 57   1   4   1   1   6   1   2   2]\n",
      " [  4  86   0   3   3   8   2   0   4]\n",
      " [  1   0  79   1   2   6   0   8   2]\n",
      " [  2   1   1  71   2   4   1   0   0]\n",
      " [  2   8   3   1  49  16   2   3  21]\n",
      " [  3   3   3   4   5  69   0   1   2]\n",
      " [  0   0   0   1   0   4  95   0   1]\n",
      " [  2   0   5   0   1   2   0 116   0]\n",
      " [  0   7   1   1   8   4   0   0  91]]\n",
      "[[62  1  4  0  2  4  0  0  2]\n",
      " [ 6 89  0  4  4  3  1  0  3]\n",
      " [ 3  0 80  3  3  5  0  3  2]\n",
      " [ 4  1  1 72  3  0  1  0  0]\n",
      " [ 2  7  4  2 72  3  0  0 15]\n",
      " [ 4 10  3 11 10 50  0  0  2]\n",
      " [ 0  1  0  1  0  3 94  0  2]\n",
      " [ 8  0 19  0  3  5  1 89  1]\n",
      " [ 1  6  1  1 15  2  0  0 86]]\n",
      "[[ 58   4   4   0   1   4   1   1   2]\n",
      " [  4  90   0   3   2   3   3   0   5]\n",
      " [  1   0  83   2   3   1   0   7   2]\n",
      " [  4   1   1  72   3   0   1   0   0]\n",
      " [  4   9   4   1  59   5   2   0  21]\n",
      " [  5   9   3   4   7  55   4   1   2]\n",
      " [  0   1   0   1   0   2  96   0   1]\n",
      " [  2   0   5   0   1   1   0 117   0]\n",
      " [  1   5   1   1   7   1   0   0  96]]\n",
      "[[ 57   2   4   0   2   6   1   1   2]\n",
      " [  5  85   0   3   2   9   2   0   4]\n",
      " [  1   0  89   1   1   2   0   3   2]\n",
      " [  2   1   1  71   3   3   1   0   0]\n",
      " [  2   8   6   1  63   8   2   0  15]\n",
      " [  1   4   3   4   6  69   1   0   2]\n",
      " [  0   0   0   1   0   3  96   0   1]\n",
      " [  2   0  12   0   1   1   1 109   0]\n",
      " [  1   5   1   1  13   2   0   0  89]]\n",
      "[[ 59   2   4   0   1   5   1   1   2]\n",
      " [  6  85   0   1   3  10   2   0   3]\n",
      " [  2   0  82   1   3   2   0   7   2]\n",
      " [  4   1   1  69   4   2   1   0   0]\n",
      " [  3   6   3   1  70   5   1   1  15]\n",
      " [  4   2   3   3   8  66   3   0   1]\n",
      " [  0   0   0   1   0   2  97   0   1]\n",
      " [  2   0   5   0   1   0   0 118   0]\n",
      " [  1   5   1   1  12   2   0   0  90]]\n",
      "[[ 60   1   4   0   1   4   1   1   3]\n",
      " [  5  85   0   4   2   7   3   0   4]\n",
      " [  1   0  86   2   2   1   0   5   2]\n",
      " [  4   1   1  72   2   0   1   0   1]\n",
      " [  3   6   4   1  62   4   1   0  24]\n",
      " [  4   6   3   6   6  58   4   0   3]\n",
      " [  0   0   0   1   0   2  97   0   1]\n",
      " [  2   0   7   0   1   1   1 114   0]\n",
      " [  1   5   1   1   8   2   0   0  94]]\n",
      "[[ 59   3   4   0   1   4   1   1   2]\n",
      " [  5  87   0   4   2   5   3   0   4]\n",
      " [  1   0  86   2   2   1   0   5   2]\n",
      " [  4   1   1  72   3   0   1   0   0]\n",
      " [  3   6   4   1  64   4   1   0  22]\n",
      " [  4   7   3   4   6  59   4   0   3]\n",
      " [  0   0   0   1   0   2  97   0   1]\n",
      " [  2   0   8   0   1   1   1 113   0]\n",
      " [  1   5   1   1   9   2   0   0  93]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=640, training_loss=0.6814454078674317, metrics={'train_runtime': 1787.9271, 'train_samples_per_second': 45.304, 'train_steps_per_second': 0.358, 'total_flos': 5.12290727165952e+17, 'train_loss': 0.6814454078674317, 'epoch': 10.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWwOb9Gow-rL"
   },
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Gn32rrItAp4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[137   0  27   9   0  16   0   0  11]\n",
      " [ 16  50   2   2   0  53   0   0  77]\n",
      " [  2   0 183   0   0   4   0   0  11]\n",
      " [ 11   2  26 111   0  25   1   1  23]\n",
      " [ 24   2  22   3   1  28   5   1 114]\n",
      " [ 14   3  39   8   0  90   2   1  43]\n",
      " [  3   0  24   0   0  14 143   2  14]\n",
      " [  2   0 145   0   0   1   3  45   4]\n",
      " [  0   0   1   0   0   1   0   1 197]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9156763553619385,\n",
       " 'eval_accuracy': 0.5316666666666666,\n",
       " 'eval_f1': 0.49262213079186995,\n",
       " 'eval_precision': 0.706123840883612,\n",
       " 'eval_recall': 0.5316666666666667,\n",
       " 'eval_runtime': 33.2014,\n",
       " 'eval_samples_per_second': 54.215,\n",
       " 'eval_steps_per_second': 0.452,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
